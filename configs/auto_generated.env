MODEL_NAME=qwen2.5-3b-instruct-q4_k_m.gguf
LLAMA_IMAGE=ghcr.io/ggerganov/llama.cpp:server
THREADS=8
BATCH_SIZE=512
CTX_SIZE=4096
GPU_LAYERS=0
N_PREDICT=256
REPEAT_PENALTY=1.1
TEMP=0.7
TOP_P=0.9
TOP_K=40
HOST_RAM_GB=16
HOST_CPU_COUNT=8
